{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53b78296",
   "metadata": {},
   "source": [
    "First load some utility modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b079fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stemutils.io import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be4960",
   "metadata": {},
   "source": [
    "Next we define the bulk of the script that we are going to modify for each training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ac95224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_script_text(dp, model_path, hparams, folder_name):\n",
    "    return f'''\n",
    "\n",
    "\n",
    "\n",
    "#load some packages in\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as python_random\n",
    "from numba import njit\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from stemutils.io import Path\n",
    "import hyperspy.api as hs\n",
    "import concurrent.futures\n",
    "from skimage.transform import resize\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import lru_cache\n",
    "from stemseg.processing_funcs import *\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "#set some variables\n",
    "print('Using TensorFlow v%s' % tf.__version__)\n",
    "plt.style.use('default')\n",
    "python_random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "\n",
    "#define some functions\n",
    "\n",
    "###################################################\n",
    "########### Data Preprocessing ####################\n",
    "###################################################\n",
    "\n",
    "def batch_resize(d, bs=512):\n",
    "    if len(d.shape) == 4:\n",
    "        flat_d = flatten_nav(d)\n",
    "    else:\n",
    "        flat_d = d\n",
    "    n_batches = int(np.ceil(flat_d.shape[0]//bs))\n",
    "    batches = [flat_d[i*bs:(i+1)*bs] for i in range(n_batches+1)]\n",
    "    if len(batches[-1])==0:\n",
    "        batches.pop(-1)\n",
    "    print(len(batches[-1]))\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=4) as exe:\n",
    "        res = [exe.submit(resize, batch, (batch.shape[0],128,128)) for batch in batches]\n",
    "    r_batches = [f.result() for f in res]\n",
    "    return np.concatenate(r_batches, axis = 0).reshape((d.shape[0],128,128))\n",
    "\n",
    "def data_manip(d, bs = 512):\n",
    "    if type(d) != np.ndarray:\n",
    "        print('dask to numpy')\n",
    "        d = d.compute()\n",
    "        print('dask to numpy done')\n",
    "    print('started data manipulations')\n",
    "    #d = resize(d,(d.shape[0],128,128))\n",
    "    print('resized')\n",
    "    d = d.astype('float32')\n",
    "    for i in range(d.shape[0]):\n",
    "        d_max = np.max(d[i])\n",
    "        d[i] = d[i]/d_max\n",
    "    d = batch_resize(d, bs)\n",
    "    scaler = np.log(1001)\n",
    "    return np.log((d*1000)+1)/scaler \n",
    "\n",
    "def data_manip_lowq(d, central_box = 128):\n",
    "    pxc, pyc = d.shape[1]//2, d.shape[2]//2 \n",
    "    pxl, pxu = pxc - central_box//2, pxc + central_box//2 \n",
    "    pyl, pyu = pyc - central_box//2, pyc + central_box//2 \n",
    "    \n",
    "    d = d[:, pxl:pxu, pyl:pyu]\n",
    "    if type(d) != np.ndarray:\n",
    "        print('dask to numpy')\n",
    "        d = d.compute()\n",
    "        print('dask to numpy done')\n",
    "    print('started data manipulations')\n",
    "    #d = resize(d,(d.shape[0],128,128))\n",
    "    print('resized')\n",
    "    d = d.astype('float32')\n",
    "    for i in range(d.shape[0]):\n",
    "        d_max = np.max(d[i])\n",
    "        d[i] = d[i]/d_max\n",
    "    \n",
    "    scaler = np.log(1001)\n",
    "    return np.log((d*1000)+1)/scaler \n",
    "\n",
    "\n",
    "\n",
    "###################################################\n",
    "###################################################\n",
    "###################################################\n",
    "\n",
    "def flatten_nav(sig):\n",
    "    shape = [sig.shape[0]*sig.shape[1]]\n",
    "    for i in sig.shape[2:]:\n",
    "        shape.append(i)\n",
    "    return sig.reshape(shape)\n",
    "\n",
    "\n",
    "class My_Custom_Generator(keras.utils.Sequence) :\n",
    "    def __init__(self, image_filenames,  batch_size) :\n",
    "        self.image_filenames = image_filenames\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self) :\n",
    "        return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(np.int)\n",
    "    \n",
    "    \n",
    "    @lru_cache(None)\n",
    "    def __getitem__(self, idx) :\n",
    "        batch_x = self.image_filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        out_img = np.asarray([np.load(file_name)[:,:,None] for file_name in batch_x])\n",
    "        return out_img, out_img\n",
    "        #return batch_x, batch_y\n",
    "        \n",
    "        \n",
    "class Array_Generator(keras.utils.Sequence) :\n",
    "    def __init__(self, images,  batch_size) :\n",
    "        self.images = images\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self) :\n",
    "        return (np.ceil(len(self.images) / float(self.batch_size))).astype(np.int)\n",
    "    \n",
    "    \n",
    "    @lru_cache(None)\n",
    "    def __getitem__(self, idx) :\n",
    "        out_img = self.images[idx * self.batch_size : (idx+1) * self.batch_size, :,:,None]\n",
    "        return out_img, out_img\n",
    "        #return batch_x, batch_y\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        epsilon = tf.keras.backend.random_normal(shape=tf.shape(z_mean))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "def create_vae_model(hparams):\n",
    "    \n",
    "    n_img = 128\n",
    "    latent_dim = hparams['LAT']\n",
    "    beta = hparams['B']\n",
    "\n",
    "    image_input = keras.Input(shape=(n_img, n_img,1), name = 'enc_input')\n",
    "    x = layers.Conv2D(hparams['KN1'],5, strides = 2, activation='relu',padding='same', input_shape=image_input.shape, name = 'enc_conv1')(image_input)\n",
    "    x = layers.Conv2D(hparams['KN2'],5, strides = 2, activation='relu',padding='same', name = 'enc_conv2')(x)\n",
    "    x = layers.Conv2D(hparams['KN3'],5, strides = 2, activation='relu',padding='same', name = 'enc_conv3')(x)\n",
    "    x = layers.Conv2D(hparams['KN4'],5, strides = 2, activation='relu',padding='same', name = 'enc_conv4')(x)\n",
    "    x = layers.Conv2D(hparams['KN5'],5, strides = 2, activation='relu',padding='same', name = 'enc_conv5')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(hparams['D1'], activation='relu', name = 'enc_d1')(x)\n",
    "    x = layers.Dense(hparams['D2'], activation=\"relu\", name = 'enc_d2_t')(x)\n",
    "    x = layers.Dense(hparams['D2'], activation=\"relu\", name = 'enc_d3_t')(x)\n",
    "    x = layers.Dense(hparams['D2'], activation=\"relu\", name = 'enc_d4_t')(x)\n",
    "    x = layers.Dense(hparams['D2'], activation=\"relu\", name = 'enc_d5_t')(x)\n",
    "    x = layers.Dense(hparams['D2'], activation=\"relu\", name = 'enc_d6_t')(x)\n",
    "    x = layers.Dense(hparams['D2'], activation=\"relu\", name = 'enc_d7_t')(x)\n",
    "    x = layers.Dense(hparams['D2'], activation=\"relu\", name = 'enc_d8_t')(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean_t\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var_t\")(x)\n",
    "    z_output = Sampling()([z_mean, z_log_var])\n",
    "    encoder_VAE = keras.Model(image_input, [z_mean, z_log_var, z_output])\n",
    "\n",
    "    z_input = keras.Input(shape=(latent_dim,), name = 'dec_input_t')\n",
    "    x = layers.Dense(hparams['D2'], activation=\"relu\", name = 'dec_d1_t')(z_input)\n",
    "    x = layers.Dense(hparams['D2'], activation=\"relu\", name = 'dec_d2')(x)\n",
    "    x = layers.Dense(hparams['D2'], activation=\"relu\", name = 'dec_d3')(x)\n",
    "    x = layers.Dense(hparams['D2'], activation=\"relu\", name = 'dec_d4')(x)\n",
    "    x = layers.Dense(hparams['D2'], activation=\"relu\", name = 'dec_d5')(x)\n",
    "    x = layers.Dense(hparams['D2'], activation=\"relu\", name = 'dec_d6')(x)\n",
    "    x = layers.Dense(hparams['D1'], activation=\"relu\", name = 'dec_d7')(x)\n",
    "    x = layers.Dense(4*4*hparams['KN5'], activation=\"relu\", name = 'dec_d8')(x)\n",
    "    x = layers.Reshape((4, 4,hparams['KN5']))(x)\n",
    "    x = layers.Conv2DTranspose(hparams['KN4'],5, strides = 2, activation='relu',padding='same', name = 'dec_conv1')(x)\n",
    "    x = layers.Conv2DTranspose(hparams['KN3'],5, strides = 2, activation='relu',padding='same', name = 'dec_conv2')(x)\n",
    "    x = layers.Conv2DTranspose(hparams['KN2'],5, strides = 2, activation='relu',padding='same', name = 'dec_conv3')(x)\n",
    "    x = layers.Conv2DTranspose(hparams['KN1'],5, strides = 2, activation='relu',padding='same', name = 'dec_conv4')(x)\n",
    "    image_output = layers.Conv2DTranspose(1,5, strides = 2, activation='sigmoid',padding='same', name = 'dec_conv5')(x)\n",
    "    #image_output = layers.Conv2DTranspose(16,3, strides = 2, activation='sigmoid',padding='same')\n",
    "    #image_output = layers.Reshape((n_img, n_img,1))(x)\n",
    "    decoder_VAE = keras.Model(z_input, image_output)\n",
    "\n",
    "    # VAE class\n",
    "    class VAE(keras.Model):\n",
    "        # constructor\n",
    "        def __init__(self, encoder, decoder, **kwargs):\n",
    "            super(VAE, self).__init__(**kwargs)\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "\n",
    "        # customise train_step() to implement the loss \n",
    "        def train_step(self, x):\n",
    "            if isinstance(x, tuple):\n",
    "                x = x[0]\n",
    "            with tf.GradientTape() as tape:\n",
    "                # encoding\n",
    "                z_mean, z_log_var, z = self.encoder(x)\n",
    "                # decoding\n",
    "                x_prime = self.decoder(z)\n",
    "                # reconstruction error by binary crossentropy loss\n",
    "                reconstruction_loss = tf.reduce_mean(keras.losses.binary_crossentropy(x, x_prime)) * n_img * n_img\n",
    "                # KL divergence\n",
    "                kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "                # loss = reconstruction error + KL divergence\n",
    "                loss = reconstruction_loss + beta* kl_loss\n",
    "            # apply gradient\n",
    "            grads = tape.gradient(loss, self.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "            # return loss for metrics log\n",
    "            return {'{\"loss\": loss}'}\n",
    "\n",
    "\n",
    "        def call(self, x):\n",
    "            if isinstance(x, tuple):\n",
    "                x = x[0]\n",
    "            # encoding\n",
    "            z_mean, z_log_var, z = self.encoder(x)\n",
    "            # decoding\n",
    "            x_prime = self.decoder(z)\n",
    "            return x_prime\n",
    "    # build the VAE\n",
    "    vae_model = VAE(encoder_VAE, decoder_VAE)\n",
    "\n",
    "    # compile the VAE\n",
    "    vae_model.compile(optimizer=keras.optimizers.Adam(learning_rate=hparams['LR']),loss=custom_loss)\n",
    "    vae_model.build((1,128,128,1))\n",
    "    \n",
    "    return vae_model\n",
    "\n",
    "\n",
    "\n",
    "def custom_loss(x,y):\n",
    "    n_img = 128\n",
    "    return tf.reduce_mean(keras.losses.binary_crossentropy(x, y)) * n_img * n_img\n",
    "\n",
    "def remove_background(sample, thresh = 500, old_tag=None, new_tag=None,blanker = 30):\n",
    "    d = sample.raw_data.data.copy()\n",
    "    d_shape = d.shape\n",
    "    n_shape, p_shape = d_shape[0:2], d_shape[2:]\n",
    "    ps0 = p_shape[0] //2\n",
    "    try:\n",
    "        d[:,:,ps0- blanker:ps0 + blanker, ps0 - blanker: ps0 + blanker] = np.zeros((2*blanker,2*blanker))\n",
    "    except:\n",
    "        d = d.compute()\n",
    "        d[:,:,ps0- blanker:ps0 + blanker, ps0 - blanker: ps0 + blanker] = np.zeros((2*blanker,2*blanker))\n",
    "    maskx, masky = np.where(d.sum(axis=(2,3))<thresh)\n",
    "    if old_tag !=None:\n",
    "        clustmap = sample.all_maps[old_tag].copy()\n",
    "        clustmap += 1\n",
    "        clustmap[maskx, masky] = 0 \n",
    "        newmap = np.zeros_like(clustmap)\n",
    "        for i, o in enumerate(np.unique(clustmap)):\n",
    "            newmap[np.where(clustmap == o)] = i\n",
    "        newmap += 1\n",
    "        if new_tag != None:\n",
    "            sample.all_maps[new_tag] = newmap\n",
    "        return newmap\n",
    "    else:\n",
    "        return np.where(d.sum(axis=(2,3))<thresh, 0, 1)\n",
    "\n",
    "def show_cluster_patterns(sample, tag):\n",
    "    uis = np.unique(sample.all_maps[tag])\n",
    "    od = np.zeros((uis.size, 512, 256))\n",
    "    for x,i in enumerate(uis):\n",
    "        p = resize(sample.all_patterns[tag][x], (256,256))\n",
    "        n = resize(np.where(sample.all_maps[tag] == i, p.max(), 1), (256,256))\n",
    "        p[0,:] = p.max()\n",
    "        o = np.concatenate([n,p], axis = 0)\n",
    "        od[x] = o\n",
    "    return hs.signals.Signal2D(od)\n",
    "\n",
    "def signal_boosted_scan(sample, tag):\n",
    "    uts = np.unique(sample.all_maps[tag])\n",
    "    blank = np.zeros_like((sample.raw_data))\n",
    "    blank = blank.astype('float32')\n",
    "    for i,t in enumerate(uts):\n",
    "        print(i,t)\n",
    "        blank[np.where(sample.all_maps[tag]==t)] = sample.all_patterns[tag][i]\n",
    "    return hs.signals.Signal2D(blank)\n",
    "\n",
    "def inv_sbs(sbs, tag = 'vl_vae', sp = (0,0), return_fig = False, interactive = True, **kwargs):\n",
    "    sbsg = np.repeat(sbs.data.sum(axis= (2,3))[:,:,None],3, -1)\n",
    "    sbsg /= sbsg.max()\n",
    "    \n",
    "    def boost(array):\n",
    "        return np.log10(np.log10(array+1)+1)\n",
    "\n",
    "    def format_ax():\n",
    "        ax[0].set_frame_on(False)\n",
    "        #ax[1].set_frame_on(False)\n",
    "        ax[0].set_xticks([])\n",
    "        ax[0].set_yticks([])\n",
    "        ax[1].set_xticks([])\n",
    "        ax[1].set_yticks([])\n",
    "    fig, ax = plt.subplots(2, 1, gridspec_kw={\"{'height_ratios': [1, 2]}\"}, figsize=(8,8))\n",
    "    \n",
    "    \n",
    "    clust = sample.all_maps[tag][sp[0],sp[1]]\n",
    "\n",
    "    clust_loc = np.where(sample.all_maps[tag] == clust)\n",
    "\n",
    "    new_nav = sbsg.copy()\n",
    "\n",
    "    new_nav[clust_loc] = np.array([0.1254902 , 0.69803922, 0.66666667])\n",
    "    \n",
    "    \n",
    "    ax[0].imshow(new_nav)\n",
    "    ax[1].imshow(boost(sbs.data[sp[0],sp[1]]), cmap= 'gray', **kwargs)\n",
    "\n",
    "    format_ax()\n",
    "    \n",
    "    if interactive == True:\n",
    "    \n",
    "        global coords\n",
    "        coords = []\n",
    "\n",
    "        def onclick(event):\n",
    "            global ix, iy\n",
    "            ix, iy = np.round(event.xdata,0), np.round(event.ydata,0)\n",
    "            print(ix, iy)\n",
    "\n",
    "            coords.append((ix, iy))\n",
    "\n",
    "            ax[0].clear()\n",
    "            ax[1].clear()\n",
    "\n",
    "            clust = sample.all_maps[tag][int(iy),int(ix)]\n",
    "\n",
    "            clust_loc = np.where(sample.all_maps[tag] == clust)\n",
    "\n",
    "            new_nav = sbsg.copy()\n",
    "\n",
    "            new_nav[clust_loc] = np.array([0.1254902 , 0.69803922, 0.66666667])\n",
    "\n",
    "\n",
    "\n",
    "            ax[0].imshow(new_nav)\n",
    "            ax[1].imshow(boost(sbs.data[int(iy),int(ix)]), cmap = 'gray', **kwargs)\n",
    "\n",
    "            format_ax()\n",
    "\n",
    "            ax[0].draw()\n",
    "            ax[1].draw()\n",
    "\n",
    "\n",
    "            return coords\n",
    "\n",
    "        cid = fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "\n",
    "    if return_fig == True:\n",
    "        return fig\n",
    "\n",
    "from skimage.metrics import structural_similarity as SSI\n",
    "from skimage.transform import PiecewiseAffineTransform, warp\n",
    "from sklearn.neighbors import NearestNeighbors as kNN\n",
    "\n",
    "def get_latgrid(sample, res=100):\n",
    "    xmin, xmax = np.floor(np.min(sample.encoded_data[:,0])), np.ceil(np.max(sample.encoded_data[:,0]))\n",
    "    ymin, ymax = np.floor(np.min(sample.encoded_data[:,1])), np.ceil(np.max(sample.encoded_data[:,1]))\n",
    "\n",
    "    latgrid_res = res\n",
    "\n",
    "    xgrid, ygrid = np.repeat(np.linspace(xmin, xmax, latgrid_res)[:,None],latgrid_res, axis = 1), np.repeat(np.linspace(ymin, ymax, latgrid_res)[None,:],latgrid_res, axis = 0)\n",
    "\n",
    "    return np.concatenate([xgrid[:,:,None], ygrid[:,:,None]],axis = 2)\n",
    "\n",
    "def get_latgrid_free(sample, xmin, xmax, ymin, ymax, res=100):\n",
    "    latgrid_res = res\n",
    "\n",
    "    xgrid, ygrid = np.repeat(np.linspace(xmin, xmax, latgrid_res)[:,None],latgrid_res, axis = 1), np.repeat(np.linspace(ymin, ymax, latgrid_res)[None,:],latgrid_res, axis = 0)\n",
    "\n",
    "    return np.concatenate([xgrid[:,:,None], ygrid[:,:,None]],axis = 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batch_calc_grad(img, radial_kernel, decoded_data, weighting_func, bs=256):\n",
    "    \n",
    "    ssi_ff = []\n",
    "    n_batches = int(np.ceil(img.shape[0]//bs))\n",
    "    batches = [img[i*bs:(i+1)*bs] for i in range(n_batches+1)]\n",
    "    dec_batches = [decoded_data[i*bs:(i+1)*bs] for i in range(n_batches+1)]\n",
    "    rs_batches = [b.reshape(b.shape[0]*b.shape[1]) for b in batches]\n",
    "    cart_rs_batches = [np.concatenate([b.real[:,None], b.imag[:,None]], axis = 1) for b in rs_batches]\n",
    "    for i, batch in enumerate(cart_rs_batches):\n",
    "        t1 = time.time()\n",
    "        print(i, n_batches)\n",
    "        nimg = sample.model.decoder(batch).numpy()\n",
    "        rs_nimg = nimg.reshape((int(nimg.size/(img.shape[1]*128*128)), img.shape[1], 128, 128))\n",
    "        comp_patterns = dec_batches[i]\n",
    "        for x, dec_pat in enumerate(comp_patterns):\n",
    "            grad_ssi = np.asarray([weighting_func(dec_pat, y) for y in rs_nimg[x]])\n",
    "            ssi_ff.append(np.sum(grad_ssi*radial_kernel))\n",
    "        print(time.time()-t1, 'single thread')\n",
    "    return np.asarray(ssi_ff)\n",
    "\n",
    "\n",
    "def SSI_weighting(img1, img2):\n",
    "    return 100*SSI(img1,img2)\n",
    "\n",
    "def get_mobile_points(nn_comp_enc,steps, prev_mp_locs = (), thresh = 'mean', relative_locs = False):\n",
    "    if thresh == 'mean':\n",
    "        thresh = np.mean(np.abs(steps))\n",
    "    if thresh == 'ten':\n",
    "        thresh = np.max(np.abs(steps))/10\n",
    "        print(thresh)\n",
    "        print(np.where(np.abs(steps) > thresh))\n",
    "    mp_locs = np.where(np.abs(steps) > thresh)\n",
    "    mobile_points = nn_comp_enc[mp_locs]\n",
    "    \n",
    "    if len(prev_mp_locs) != 0:\n",
    "        n_mp_locs = prev_mp_locs[mp_locs]\n",
    "        \n",
    "    if relative_locs == True:\n",
    "\n",
    "        return mobile_points, n_mp_locs, mp_locs\n",
    "    else:\n",
    "        return mobile_points, n_mp_locs\n",
    "\n",
    "def get_grad_and_decode_data(mobile_points, radial_kernel, r_scale_kernel = False, nn_scale = False):\n",
    "    if r_scale_kernel ==False:\n",
    "        grad_points = np.repeat(mobile_points[:,None], radial_kernel.shape[0], axis = 1) + radial_kernel[None, :]\n",
    "    else:\n",
    "        if nn_scale == False:\n",
    "            rf = np.round((np.abs(mobile_points)/np.abs(mobile_points).min()),0).astype('int')\n",
    "            grad_points = np.repeat(mobile_points[:,None], radial_kernel.shape[0], axis = 1) + r_scale_kernel*rf[:,None]*radial_kernel[None, :]\n",
    "        else:\n",
    "            sample_locs = np.concatenate((mobile_points.real[:,None], mobile_points.imag[:,None]), axis = 0)\n",
    "            nbrs = kNN(n_neighbors=1, algorithm='ball_tree').fit(sample_locs)\n",
    "            p_sep, indices = nbrs.kneighbors(sample_locs)\n",
    "            closest = p_sep.min()\n",
    "            norm_sep = p_sep/closest\n",
    "            grad_points = np.repeat(mobile_points[:,None], radial_kernel.shape[0], axis = 1) + r_scale_kernel*norm_sep[:,None]*radial_kernel[None, :]\n",
    "\n",
    "    dec_dat = get_terr_patts(np.concatenate([mobile_points.real[:,None], mobile_points.imag[:,None]],axis = 1))\n",
    "    return grad_points, dec_dat\n",
    "\n",
    "def sig_step_from_grad(d_gp, gradient_step, sigz=0.25, sigf=100):\n",
    "    grad_mag = np.abs(d_gp)\n",
    "\n",
    "    return sigmoid(grad_mag, sigz, sigf)*gradient_step*(d_gp/grad_mag) \n",
    "\n",
    "def norm_step_from_grad(d_gp, factor):\n",
    "    grad_mag = np.max(np.abs(d_gp))\n",
    "    \n",
    "    return (d_gp/grad_mag)*factor \n",
    "\n",
    "def sigmoid(z, sigz=0.25, sigf=100):\n",
    "    x = sigf*(z - sigz)\n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "\n",
    "\n",
    "def adjust_encoding(mobile_points, grads, comp_enc, mp_locs):\n",
    "    X,Y  = mobile_points.real, mobile_points.imag\n",
    "\n",
    "    dX, dY = grads.real, grads.imag\n",
    "    U, V = X+dX, Y+dY\n",
    "\n",
    "    moved_points = U+1j*V\n",
    "\n",
    "    migrated_points = comp_enc.copy()\n",
    "\n",
    "    migrated_points[mp_locs] = moved_points\n",
    "    \n",
    "    return (X,Y), (U,V), migrated_points\n",
    "\n",
    "def get_terr_patts(img, bs =256):\n",
    "    n_batches = int(np.ceil(img.shape[0]//bs))\n",
    "    batches = [img[i*bs:(i+1)*bs] for i in range(n_batches+1)]\n",
    "    nimg = [sample.model.decoder(batch).numpy() for batch in batches]\n",
    "    return np.concatenate(nimg, axis = 0).reshape((img.shape[0], 128,128))\n",
    "\n",
    "\n",
    "def sig_step_from_grad(d_gp, gradient_step, sigz=0.25, sigf=100):\n",
    "    grad_mag = np.abs(d_gp)\n",
    "\n",
    "    return sigmoid(grad_mag, sigz, sigf)*gradient_step*(d_gp/grad_mag) \n",
    "\n",
    "def sigmoid(z, sigz=0.25, sigf=100):\n",
    "    x = sigf*(z - sigz)\n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "\n",
    "def lin_thresh_step(d_gp, thresh, mag = 1):\n",
    "    scale = np.abs(d_gp)\n",
    "    return (np.where(scale>thresh, thresh, scale)/thresh)*(d_gp/scale)*mag\n",
    "\n",
    "def scaled_thresh_step(d_gp, thresh, mobile_points, mag):\n",
    "    sample_locs = np.concatenate((mobile_points.real[:,None], mobile_points.imag[:,None]), axis = 1)\n",
    "    nbrs = kNN(n_neighbors=5, algorithm='ball_tree').fit(sample_locs)\n",
    "    p_sep, indices = nbrs.kneighbors(sample_locs, n_neighbors = 2)\n",
    "    print(p_sep.shape, p_sep[:,0])\n",
    "    p_sep = p_sep[:,1]\n",
    "    closest = p_sep.min()\n",
    "    norm_sep = p_sep/closest\n",
    "    \n",
    "    scale = np.abs(d_gp)\n",
    "    return (np.where(scale>thresh, thresh, scale)/thresh)*(d_gp/scale)*norm_sep*mag\n",
    "\n",
    "def sig_step_from_grad(d_gp, gradient_step, sigz=0.25, sigf=100):\n",
    "    grad_mag = np.abs(d_gp)\n",
    "\n",
    "    return sigmoid(grad_mag, sigz, sigf)*gradient_step*(d_gp/grad_mag) \n",
    "\n",
    "def sigmoid(z, sigz=0.25, sigf=100):\n",
    "    x = sigf*(z - sigz)\n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "\n",
    "def lin_thresh_step(d_gp, thresh, mag = 1):\n",
    "    scale = np.abs(d_gp)\n",
    "    return (np.where(scale>thresh, thresh, scale)/thresh)*(d_gp/scale)*mag\n",
    "\n",
    "def scaled_thresh_step(d_gp, thresh, mobile_points, mag):\n",
    "    sample_locs = np.concatenate((mobile_points.real[:,None], mobile_points.imag[:,None]), axis = 1)\n",
    "    nbrs = kNN(n_neighbors=5, algorithm='ball_tree').fit(sample_locs)\n",
    "    p_sep, indices = nbrs.kneighbors(sample_locs, n_neighbors = 2)\n",
    "    print(p_sep.shape, p_sep[:,0])\n",
    "    p_sep = p_sep[:,1]\n",
    "    closest = p_sep.min()\n",
    "    norm_sep = p_sep/closest\n",
    "    \n",
    "    scale = np.abs(d_gp)\n",
    "    return (np.where(scale>thresh, thresh, scale)/thresh)*(d_gp/scale)*norm_sep*mag\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "def get_density_net(sample, n_samples, n_bkg_samples, density_approx = 10,  bandwidth=0.5):\n",
    "    D = sample.encoded_data.copy()\n",
    "    np.random.shuffle(D)\n",
    "    D = D[::density_approx]\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(D)\n",
    "    R = kde.sample(n_samples)\n",
    "    \n",
    "    \n",
    "    \n",
    "    xmin, xmax = np.floor(np.min(sample.encoded_data[:,0])), np.ceil(np.max(sample.encoded_data[:,0]))\n",
    "    ymin, ymax = np.floor(np.min(sample.encoded_data[:,1])), np.ceil(np.max(sample.encoded_data[:,1]))\n",
    "    \n",
    "    print(xmin, xmax,ymin,ymax)\n",
    "    s_samples = np.random.random((n_bkg_samples, 2))\n",
    "    \n",
    "    s_samples[:,0] *= np.abs((xmax - xmin))\n",
    "    s_samples[:,1] *= np.abs((ymax - ymin))\n",
    "    s_samples = s_samples + np.array((xmin, ymin))\n",
    "    \n",
    "    return np.concatenate((R, s_samples), axis = 0)\n",
    "\n",
    "import sklearn.metrics.cluster as cmet\n",
    "\n",
    "def get_map_label_df(map1):\n",
    "    return np.asarray([np.where(map1 == uinds, 1, 0) for uinds in np.unique(map1)])\n",
    "\n",
    "def get_cluster_label_overlap(map_pair):\n",
    "    db1_df,db2_df = map_pair\n",
    "    label_overlap = np.zeros((db1_df.shape[0], db2_df.shape[0]))\n",
    "    for i, idf in enumerate(db1_df):\n",
    "        for j, jdf in enumerate(db2_df):\n",
    "            label_overlap[i,j] = np.sum(db1_df[i] * db2_df[j])/ np.sum(db1_df[i])\n",
    "    return label_overlap\n",
    "\n",
    "def find_map_label(pos, map1):\n",
    "    return map1[pos]\n",
    "\n",
    "def get_confidence_from_maps(maps):\n",
    "    dfs = [x for x in map(get_map_label_df, maps)]\n",
    "\n",
    "    cluster_overlaps = [x for x in map(get_cluster_label_overlap, [x for x in itertools.permutations(dfs, 2)])]\n",
    "\n",
    "    overlap_inds = [x for x in itertools.permutations(np.arange(len(maps)), 2)]\n",
    "\n",
    "    overlap_inds\n",
    "\n",
    "    len(cluster_overlaps)\n",
    "\n",
    "    confidence = np.zeros_like(map1, dtype='float32')\n",
    "    for point in range(len(map1)):\n",
    "        labels = [i for i in map(find_map_label, np.repeat(point, len(maps)) , maps)]\n",
    "        total = 0\n",
    "        for cind, oinds in enumerate(overlap_inds):\n",
    "            l1, l2 = labels[oinds[0]], labels[oinds[1]]\n",
    "            total+=cluster_overlaps[cind][l1, l2]\n",
    "        mean = total/len(overlap_inds)\n",
    "        confidence[point] = mean\n",
    "    return confidence\n",
    "\n",
    "\n",
    "\n",
    "def refine_based_on_density(sample, density_cutoff = -9, n_bulk_samples = 500, sample_grid_res = 200,\n",
    "                            gn = 0.1,density_approx = 5, bw = 0.4, n_sample_points = 2500, n_bkg_points = 500, \n",
    "                            show_net = True, rand_gradient_step = 0.01,rand_n_rsteps = 12, step_scale = 0.01, \n",
    "                            step_thresh = 0.001,show_step_size = True, show_net_movement= True, \n",
    "                            show_first_refinement = False,n_refine_steps = 200, show_refinement = True, \n",
    "                            animate_refinement = True):\n",
    "    \n",
    "    #create a dictionary to hold some data that might be useful to return\n",
    "    accessory_dict = {\"{}\"}\n",
    "    \n",
    "    #get a density based net\n",
    "    #R = get_density_net(sample, n_sample_points, n_bkg_points, density_approx, bw)\n",
    "    R = get_density_gradient_net(sample, n_sample_points, density_cutoff, n_bkg_points,n_bulk_samples, density_approx, sample_grid_res,bw, gn)\n",
    "    \n",
    "    accessory_dict['net'] = R\n",
    "    \n",
    "    #view the point distribution\n",
    "    if show_net == True:\n",
    "        plt.figure()\n",
    "        plt.scatter(sample.encoded_data[:,0], sample.encoded_data[:,1], s = 10)\n",
    "        plt.scatter(R[:,0], R[:,1], s = 20)\n",
    "    \n",
    "    #get all the sample points for the gradient \n",
    "    rand_latspace = R[:,0] + 1j*R[:,1]\n",
    "    rand_radial_kernel = rand_gradient_step* np.exp(1j*np.pi*(np.linspace(0, 360, rand_n_rsteps+1)/180))[1:]\n",
    "    rand_grad_p1, rand_decdat1 = get_grad_and_decode_data(rand_latspace, rand_radial_kernel)\n",
    "    \n",
    "    #calculate the gradients\n",
    "    rand_delta_gp1 = batch_calc_grad(rand_grad_p1, rand_radial_kernel, rand_decdat1, SSI_weighting, 256)\n",
    "    accessory_dict['grads'] = rand_delta_gp1\n",
    "    \n",
    "    #scale the gradients for step sizes\n",
    "    rand_linsteps = lin_thresh_step(rand_delta_gp1, step_thresh, step_scale)\n",
    "    if show_step_size == True:\n",
    "        plt.figure()\n",
    "        plt.plot(np.sort(np.abs(rand_linsteps)))\n",
    "    accessory_dict['steps'] = rand_linsteps\n",
    "    \n",
    "    #adjust the net points\n",
    "    rand_op1, rand_np1, rand_current_ps1= adjust_encoding(rand_latspace, rand_linsteps, rand_latspace, np.where(rand_latspace != None))\n",
    "    accessory_dict['net_displacement'] = rand_np1\n",
    "    if show_net_movement == True:\n",
    "        plt.figure(figsize = (8,8))\n",
    "        plt.scatter(rand_op1[0], rand_op1[1],s =20)\n",
    "        plt.scatter(rand_np1[0], rand_np1[1],s =20)\n",
    "        plt.scatter(sample.encoded_data[:,0], sample.encoded_data[:,1], s = 10, alpha = 0.2)\n",
    "    #refine the points once\n",
    "    test_data = sample.encoded_data.copy()\n",
    "\n",
    "    rand_o_latspacer = np.asarray(rand_op1).T\n",
    "    rand_n_latspacer = np.asarray(rand_np1).T\n",
    "\n",
    "    rand_tform2 = PiecewiseAffineTransform()\n",
    "    accessory_dict['transform'] = rand_tform2\n",
    "    rand_tform2.estimate(rand_n_latspacer, rand_o_latspacer)\n",
    "\n",
    "    rand_out_data2 = rand_tform2.inverse(test_data)\n",
    "    \n",
    "    accessory_dict['first_refinement'] = rand_out_data2.copy()\n",
    "\n",
    "    if show_first_refinement == True:\n",
    "        plt.figure()\n",
    "        plt.scatter(test_data[:,0], test_data[:,1], s =10)\n",
    "        plt.scatter(rand_out_data2[:,0], rand_out_data2[:,1], s =10)\n",
    "        \n",
    "    rand_refine_steps2=[]\n",
    "\n",
    "    for i in range(n_refine_steps):\n",
    "        rand_out_data2 = rand_tform2.inverse(rand_out_data2)\n",
    "        rand_refine_steps2.append(rand_out_data2)\n",
    "    \n",
    "    if show_refinement == True:\n",
    "        plt.figure()\n",
    "        plt.scatter(test_data[:,0], test_data[:,1], s =10)\n",
    "        #plt.scatter(out_data[:,0], out_data[:,1], s =10)\n",
    "        plt.scatter(rand_out_data2[:,0], rand_out_data2[:,1], s =10)\n",
    "        \n",
    "    accessory_dict['refinement_steps'] = rand_refine_steps2\n",
    "\n",
    "    \n",
    "    if animate_refinement == True:\n",
    "        # First set up the figure, the axis, and the plot element we want to animate\n",
    "        figr3, axr3 = plt.subplots()\n",
    "\n",
    "        axr3.set_xlim(( -1, 1))\n",
    "        axr3.set_ylim((-1, 1))\n",
    "\n",
    "        liner3, = axr3.plot([], [], lw=2, ls = '', marker = 'o', alpha = 0.2)\n",
    "\n",
    "        def init3():\n",
    "            liner3.set_data([], [])\n",
    "            return (liner,)\n",
    "        def animate3(i):\n",
    "            d3 = rand_refine_steps2[i]\n",
    "            x3,y3 = d3[:,0], d3[:,1]\n",
    "            liner3.set_data(x3, y3)\n",
    "            return (liner3,)\n",
    "        # call the animator. blit=True means only re-draw the parts that \n",
    "        # have changed.\n",
    "        animr3 = animation.FuncAnimation(figr3, animate3, init_func=init3,\n",
    "                                       frames=1250, interval=20, blit=True)\n",
    "        accessory_dict['animation'] = animr3\n",
    "    return rand_out_data2, accessory_dict\n",
    "\n",
    "def get_density_gradient_net(D, n_samples, density_cutoff, n_bkg_samples, n_bulk_samples, density_approx = 10, sample_grid_res = 200, bandwidth=0.5, gn = 0.1):\n",
    "    np.random.shuffle(D)\n",
    "    D = D[::density_approx]\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(D)\n",
    "    R = kde.sample(n_bulk_samples)\n",
    "    \n",
    "    \n",
    "    xgrid = np.linspace(np.floor(D[:,0].min()),np.ceil(D[:,0].max()),sample_grid_res)\n",
    "    ygrid = np.linspace(np.floor(D[:,1].min()),np.ceil(D[:,1].max()),sample_grid_res)\n",
    "    X,Y = np.meshgrid(xgrid, ygrid)\n",
    "    xy = np.vstack((X.ravel(), Y.ravel())).T\n",
    "\n",
    "    Z = kde.score_samples(xy).reshape(X.shape)\n",
    "\n",
    "    dY, dX = np.gradient(Z)\n",
    "    \n",
    "    dZ = np.hypot(dY,dX)*np.where(Z < density_cutoff, 0, 1)*np.where(Z> (density_cutoff+3), 0, 1)\n",
    "\n",
    "    dZ = dZ/np.sum(dZ)\n",
    "\n",
    "    dZ = dZ.reshape(xy.shape[0])\n",
    "\n",
    "    draw = np.random.choice(np.arange(xy.shape[0]), n_samples,\n",
    "                  p=dZ, replace = True)\n",
    "\n",
    "    kdgrad = xy[draw] + gn*(np.random.random((n_samples, 2))-0.5*np.ones((n_samples, 2)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    xmin, xmax = np.floor(np.min(sample.encoded_data[:,0])), np.ceil(np.max(sample.encoded_data[:,0]))\n",
    "    ymin, ymax = np.floor(np.min(sample.encoded_data[:,1])), np.ceil(np.max(sample.encoded_data[:,1]))\n",
    "    \n",
    "    print(xmin, xmax,ymin,ymax)\n",
    "    \n",
    "    bbox = np.array(((xmin, ymin), (xmin, ymax), (xmax, ymin), (xmax, ymax)))\n",
    "    s_samples = np.random.random((n_bkg_samples, 2))\n",
    "    \n",
    "    s_samples[:,0] *= np.abs((xmax - xmin))\n",
    "    s_samples[:,1] *= np.abs((ymax - ymin))\n",
    "    s_samples = s_samples + np.array((xmin, ymin))\n",
    "    \n",
    "    return np.concatenate((kdgrad, s_samples, R), axis = 0)\n",
    "\n",
    "def SSI_remesh(sample, R, n_add_points = 1, ssi_thresh = 0.95):\n",
    "    tri = Delaunay(R)\n",
    "\n",
    "    all_simps = tri.simplices\n",
    "\n",
    "    line_segs= np.asarray([[np.asarray(x) for x in itertools.combinations(R[simps], 2)] for simps in all_simps])\n",
    "\n",
    "    line_add_points= np.asarray([np.asarray([line_interp(x[0], x[1],2+n_add_points)[1:-1] for x in itertools.combinations(R[simps], 2)]) for simps in all_simps])\n",
    "\n",
    "    f_line_segs = flatten_nav(line_segs)\n",
    "\n",
    "    f_add_points = flatten_nav(line_add_points)\n",
    "\n",
    "    fline_start, fline_finish = f_line_segs[:,0,:], f_line_segs[:,1,:] \n",
    "\n",
    "    patts_start, patts_finish = get_terr_patts(fline_start), get_terr_patts(fline_finish)\n",
    "\n",
    "    line_ssi = np.zeros(patts_start.shape[0])\n",
    "\n",
    "    ssi_input_data = []\n",
    "    for i in range(patts_start.shape[0]):\n",
    "        ssi_input_data.append((i, patts_start[i], patts_finish[i]))\n",
    "        \n",
    "    print('getting line ssi')\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor() as exe:\n",
    "        res = [exe.submit(compare_point_SSI, ssi_input) for ssi_input in ssi_input_data]\n",
    "    r_batches = [f.result() for f in res]\n",
    "    \n",
    "    print('done getting line ssi')\n",
    "\n",
    "    for each in r_batches:\n",
    "        line_ssi[each[0]] = each[1]\n",
    "\n",
    "    poor_line_locs = np.where(line_ssi < ssi_thresh)\n",
    "\n",
    "    new_points = flatten_nav(f_add_points[poor_line_locs])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(sample.encoded_data[:,0], sample.encoded_data[:,1], s = 10, alpha = 0.5, c = 'grey')\n",
    "    plt.triplot(R[:,0], R[:,1], all_simps, lw = 1)\n",
    "    plt.scatter(new_points[:,0], new_points[:,1], s = 10, alpha = 1, c = 'black', marker = 'x')\n",
    "    plt.title('Additional Mesh Points')\n",
    "    \n",
    "    Rp = np.concatenate((R, new_points), axis = 0)\n",
    "    \n",
    "    trip = Delaunay(Rp)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(sample.encoded_data[:,0], sample.encoded_data[:,1], s = 10, alpha = 0.5, c = 'grey')\n",
    "    plt.triplot(Rp[:,0], Rp[:,1], trip.simplices, lw = 1)\n",
    "    plt.title('New Mesh')\n",
    "    return Rp\n",
    "\n",
    "def compare_point_SSI(required_data):\n",
    "    return (required_data[0], SSI(required_data[1], required_data[2]))\n",
    "\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "def latspace_from_R(R):\n",
    "    return R[:,0] + 1j*R[:,1]\n",
    "\n",
    "def get_mesh_gradients(R, rand_gradient_step, rand_n_rsteps, bs = 256):\n",
    "    rand_latspace = latspace_from_R(R)\n",
    "\n",
    "    rand_radial_kernel = rand_gradient_step* np.exp(1j*np.pi*(np.linspace(0, 360, rand_n_rsteps+1)/180))[1:]\n",
    "\n",
    "    rand_grad_p1, rand_decdat1 = get_grad_and_decode_data(rand_latspace, rand_radial_kernel)\n",
    "\n",
    "    return batch_calc_grad(rand_grad_p1, rand_radial_kernel, rand_decdat1, SSI_weighting, bs)\n",
    "\n",
    "def get_mesh_transform(R, rand_linsteps):\n",
    "    rand_latspace = latspace_from_R(R)\n",
    "    rand_op1, rand_np1, rand_current_ps1= adjust_encoding(rand_latspace, rand_linsteps, rand_latspace, np.where(rand_latspace != None))\n",
    "\n",
    "    rand_o_latspacer = np.asarray(rand_op1).T\n",
    "    rand_n_latspacer = np.asarray(rand_np1).T\n",
    "\n",
    "    rand_tform2 = PiecewiseAffineTransform()\n",
    "    rand_tform2.estimate(rand_n_latspacer, rand_o_latspacer)\n",
    "    R_moves =  (rand_np1, rand_op1)\n",
    "    return rand_tform2, R_moves\n",
    "\n",
    "def plot_R_movement(sample, R_moves):\n",
    "    X,Y = (R_moves[1][0], R_moves[1][1])\n",
    "    U, V = (R_moves[0][0] -  R_moves[1][0], R_moves[0][1]- R_moves[1][1])\n",
    "    plt.figure(figsize = (8,8))\n",
    "    plt.scatter(sample.encoded_data[:,0], sample.encoded_data[:,1], s = 10, alpha = 0.2, c = 'grey')\n",
    "    plt.scatter(X, Y ,s =10, c='blue')\n",
    "    plt.scatter(R_moves[0][0], R_moves[0][1],s =10, c ='orange')\n",
    "    plt.quiver(X, Y, U,V)\n",
    "    \n",
    "def plot_enc_movement(sample, tform):\n",
    "    rand_out_data2 = tform.inverse(sample.encoded_data)\n",
    "    plt.figure()\n",
    "    plt.scatter(sample.encoded_data[:,0], sample.encoded_data[:,1], s =10)\n",
    "    #plt.scatter(out_data[:,0], out_data[:,1], s =10)\n",
    "    plt.scatter(rand_out_data2[:,0], rand_out_data2[:,1], s =10)\n",
    "    \n",
    "def repeat_tform(tform, n_refine_steps, iter_seq):\n",
    "    rand_out_data2 = iter_seq[-1]\n",
    "    import time\n",
    "    t1 = time.time()\n",
    "    for i in range(n_refine_steps):\n",
    "        print(i)\n",
    "        rand_out_data2 = tform.inverse(rand_out_data2)\n",
    "        iter_seq.append(rand_out_data2)\n",
    "    print(time.time() - t1)\n",
    "    return iter_seq\n",
    "\n",
    "def cosine_rule(a, b, c):\n",
    "    return np.arccos(((a**2) +(b**2) - (c**2))/(2*a*b))\n",
    "\n",
    "def angles(ps):\n",
    "    p1, p2, p3 = ps\n",
    "    a, b, c = p2-p1, p3-p2, p1-p3\n",
    "    al, bl, cl = np.linalg.norm(p2-p1), np.linalg.norm(p3-p2), np.linalg.norm(p1-p3)\n",
    "    return [np.rad2deg(x) for x in [cosine_rule(al, bl, cl), cosine_rule(bl, cl, al), cosine_rule(cl,al,bl)]]\n",
    "\n",
    "def heron(ps):\n",
    "    p1, p2, p3 = ps\n",
    "    a, b, c = np.linalg.norm(p2-p1), np.linalg.norm(p3-p2), np.linalg.norm(p1-p3)\n",
    "    s = (a+b+c)/2\n",
    "    return np.sqrt(s*(s-a)*(s-b)*(s-c))\n",
    "\n",
    "def line_interp(p1, p2, nsteps):\n",
    "    return np.concatenate([np.linspace(p1[0], p2[0], nsteps)[:,None], np.linspace(p1[1], p2[1], nsteps)[:,None]], axis = 1)\n",
    "\n",
    "def remesh(sample, R, R_moves, density_cutoff, n_add_points = 1, n_movement_bins = 10, area_thresh = None, angle_thresh = 15, n_area_bins = 1000, n_line_interp = 10):\n",
    "    #get current triangulation \n",
    "    tri = Delaunay(R)\n",
    "    #view current triangulation\n",
    "    plt.figure()\n",
    "    plt.scatter(sample.encoded_data[:,0], sample.encoded_data[:,1], s = 10, alpha = 0.5, c = 'grey')\n",
    "    plt.triplot(R[:,0], R[:,1], tri.simplices, lw = 1)\n",
    "    plt.plot(R[:,0], R[:,1], 'o', markersize= 1)\n",
    "    plt.title('Initial Triangulation')\n",
    "    #get current mesh movements\n",
    "    rand_np1, rand_op1 = R_moves\n",
    "    movement = np.asarray((rand_np1[0] -  rand_op1[0], rand_np1[1]- rand_op1[1])).T\n",
    "    #get total movement of the simplex vertices\n",
    "    simp_move = np.sum(np.asarray([np.asarray([np.linalg.norm(movement[x]) for x in simp]) for simp in tri.simplices]), axis = 1)\n",
    "    #hist these\n",
    "    plt.figure()\n",
    "    (n, bins, patches) =  plt.hist(simp_move, n_movement_bins)\n",
    "    plt.title('Simplex Movement Histogram')\n",
    "    #Truncate after first bin\n",
    "    high_m_simps = tri.simplices[np.where(simp_move > bins[1])]\n",
    "    plt.figure()\n",
    "    plt.scatter(sample.encoded_data[:,0], sample.encoded_data[:,1], s = 10, alpha = 0.5, c = 'grey')\n",
    "    plt.triplot(R[:,0], R[:,1], high_m_simps, lw = 1)\n",
    "    plt.title('High Movement Simplices')\n",
    "    #Get Area of remaining simplices\n",
    "    simp_area = np.asarray([heron(R[simp]) for simp in high_m_simps])\n",
    "    simp_angles =  np.asarray([angles(R[simp]) for simp in high_m_simps])\n",
    "    plt.figure()\n",
    "    (n, area_bins, patches) =  plt.hist(simp_area, n_area_bins)\n",
    "    plt.title('Simplex Area Histogram')\n",
    "    if area_thresh == None:\n",
    "        size_inc = np.where(np.where(simp_area > area_bins[1], 1, 0) + (np.where(simp_angles[:,0] < angle_thresh, 1, 0)*np.where(simp_area > area_bins[1]/10, 1, 0))>0)\n",
    "        high_a_simps = high_m_simps[size_inc]\n",
    "    else: \n",
    "        size_inc = np.where(np.where(simp_area > area_thresh, 1, 0) + (np.where(simp_angles[:,0] < angle_thresh, 1, 0)*np.where(simp_area > area_thresh/10, 1, 0))>0)\n",
    "        high_a_simps = high_m_simps[size_inc]\n",
    "    plt.figure()\n",
    "    plt.scatter(sample.encoded_data[:,0], sample.encoded_data[:,1], s = 10, alpha = 0.5, c = 'grey')\n",
    "    plt.triplot(R[:,0], R[:,1], tri.simplices, lw = 1)\n",
    "    plt.triplot(R[:,0], R[:,1], high_a_simps, lw = 1)\n",
    "    plt.title('Area and Movement Pruned Simplices')\n",
    "    #For each of the remaining mesh lines, calculate a linear interpolation of sampling points\n",
    "    line_segs= np.asarray([np.asarray([line_interp(x[0], x[1],n_line_interp) for x in itertools.combinations(R[simps], 2)]) for simps in high_a_simps])\n",
    "    #and a midpoint to be potentially added to the new mesh\n",
    "    line_add_points= np.asarray([np.asarray([line_interp(x[0], x[1],2+n_add_points)[1:-1] for x in itertools.combinations(R[simps], 2)]) for simps in high_a_simps])\n",
    "    #calculate an approximation of real data density at each of the points along the mesh line\n",
    "    print(line_segs.shape)\n",
    "    den_seg = np.asarray([[kde.score_samples(lss) for lss in ls] for ls in line_segs])\n",
    "    #store the min and max value of this density\n",
    "    den_seg_minmax = np.concatenate((den_seg.min(axis = 2)[:,:,None], den_seg.max(axis = 2)[:,:,None]), axis = 2)\n",
    "    #calculate the gradient of the change in real data density along the simplex line\n",
    "    den_seg_grad = np.gradient(den_seg, axis = 2)\n",
    "    #Find if there is a change in sign of the density (implying a change in character of the underlying point distr)\n",
    "    #old grad_changes = np.asarray([[(lines.max() * lines.min())> 0 for lines in simps] for simps in den_seg_grad])\n",
    "    grad_changes = np.asarray([[ np.abs(lines).max() > 1 for lines in simps] for simps in den_seg_grad])\n",
    "    #If there is a change in sign and the maximum value of the density is sufficently large \n",
    "    # ie (the line itself is near points) then add the line to be split\n",
    "    interesting_lines = []\n",
    "    for si in range(line_segs.shape[0]):\n",
    "        for li in range(line_segs.shape[1]):\n",
    "            if grad_changes[si, li] == True:\n",
    "                if den_seg_minmax[si,li, 1] > density_cutoff:\n",
    "                    interesting_lines.append((si, li))\n",
    "    ilines = np.asarray(interesting_lines)\n",
    "    i_simps =  high_a_simps[np.unique(ilines[:,0])]\n",
    "    #get the midpoints of these lines\n",
    "    refinement_points = flatten_nav(np.asarray([line_add_points[ninds[0], ninds[1]] for ninds in ilines]))\n",
    "    plt.figure()\n",
    "    plt.scatter(sample.encoded_data[:,0], sample.encoded_data[:,1], s = 10, alpha = 0.5, c = 'grey')\n",
    "    plt.triplot(R[:,0], R[:,1], high_a_simps, lw = 1)\n",
    "    plt.triplot(R[:,0], R[:,1], i_simps, lw = 1)\n",
    "    plt.scatter(refinement_points[:,0], refinement_points[:,1], s = 10, alpha = 1, c = 'black', marker = 'x')\n",
    "    plt.title('Additional Mesh Points')\n",
    "    #Add these points to the original points\n",
    "    Rp = np.concatenate((R, refinement_points), axis = 0)\n",
    "    #View the new Triangulation\n",
    "    trip = Delaunay(Rp)\n",
    "    plt.figure()\n",
    "    plt.scatter(sample.encoded_data[:,0], sample.encoded_data[:,1], s = 10, alpha = 0.5, c = 'grey')\n",
    "    plt.triplot(Rp[:,0], Rp[:,1], trip.simplices, lw = 1)\n",
    "    plt.plot(Rp[:,0], Rp[:,1], 'o', markersize= 1)\n",
    "    plt.title('New Triangulation')\n",
    "    return (Rp, refinement_points)\n",
    "\n",
    "def get_dense_centroids(boundaries, allowed_centroid_mask, thresh = 0.01, eps = 1.5, min_samples = 6):\n",
    "    bdZ = get_grad(boundaries)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(bdZ)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(np.where(bdZ<thresh, 1, 0))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(np.where(bdZ<thresh, 1, 0)* allowed_centroid_mask)\n",
    "    \n",
    "    centroid_search_region = np.where(bdZ<thresh, 1, 0)* allowed_centroid_mask\n",
    "\n",
    "    density_stationary_points = np.asarray(np.where((centroid_search_region)==1)).T\n",
    "\n",
    "    db = DBSCAN(eps, min_samples=min_samples )\n",
    "\n",
    "    dspc = db.fit_predict(density_stationary_points)\n",
    "\n",
    "    dspc.max()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(density_stationary_points[:,0], density_stationary_points[:,1], c = dspc)\n",
    "\n",
    "    dsp_centroids = np.asarray([np.mean(density_stationary_points[np.where(dspc == uind)],axis = 0) for uind in np.unique(dspc) if uind != -1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(density_stationary_points[:,0], density_stationary_points[:,1], c = dspc)\n",
    "    plt.scatter(dsp_centroids[:,0], dsp_centroids[:,1], marker = 'x', c = 'red')\n",
    "\n",
    "    xgrid = np.linspace(np.floor(D[:,0].min()),np.ceil(D[:,0].max()),sample_grid_res)\n",
    "    ygrid = np.linspace(np.floor(D[:,1].min()),np.ceil(D[:,1].max()),sample_grid_res)\n",
    "\n",
    "    centroid_approx = np.round(dsp_centroids,0).astype('int')\n",
    "\n",
    "    centroid_approx\n",
    "\n",
    "    espace_dsp_centroids = np.concatenate((xgrid[centroid_approx[:,1]][:,None], ygrid[centroid_approx[:,0]][:,None]), axis = 1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(sample.encoded_data[:,0], sample.encoded_data[:,1], s=5, alpha = 0.1, cmap= 'turbo')\n",
    "    plt.scatter(espace_dsp_centroids[:,0], espace_dsp_centroids[:,1], marker='x')\n",
    "    \n",
    "    return espace_dsp_centroids, centroid_search_region\n",
    "\n",
    "\n",
    "def get_sparse_centroids(boundaries, allowed_centroid_mask, thresh = 0.01, eps = 6, min_samples = 7):\n",
    "    bdZ = get_grad(get_grad(get_grad(get_grad(boundaries))))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(bdZ)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(np.where(bdZ<thresh, 1, 0))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(np.where(bdZ<thresh, 1, 0)* allowed_centroid_mask)\n",
    "    \n",
    "    centroid_search_region = np.where(bdZ<thresh, 1, 0)* allowed_centroid_mask\n",
    "\n",
    "    density_stationary_points = np.asarray(np.where((centroid_search_region)==1)).T\n",
    "\n",
    "    db = DBSCAN(eps, min_samples=min_samples )\n",
    "\n",
    "    dspc = db.fit_predict(density_stationary_points)\n",
    "\n",
    "    dspc.max()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(density_stationary_points[:,0], density_stationary_points[:,1], c = dspc)\n",
    "\n",
    "    dsp_centroids = np.asarray([np.mean(density_stationary_points[np.where(dspc == uind)],axis = 0) for uind in np.unique(dspc) if uind != -1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(density_stationary_points[:,0], density_stationary_points[:,1], c = dspc)\n",
    "    plt.scatter(dsp_centroids[:,0], dsp_centroids[:,1], marker = 'x', c = 'red')\n",
    "\n",
    "    xgrid = np.linspace(np.floor(D[:,0].min()),np.ceil(D[:,0].max()),sample_grid_res)\n",
    "    ygrid = np.linspace(np.floor(D[:,1].min()),np.ceil(D[:,1].max()),sample_grid_res)\n",
    "\n",
    "    centroid_approx = np.round(dsp_centroids,0).astype('int')\n",
    "\n",
    "    centroid_approx\n",
    "\n",
    "    espace_dsp_centroids = np.concatenate((xgrid[centroid_approx[:,1]][:,None], ygrid[centroid_approx[:,0]][:,None]), axis = 1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(sample.encoded_data[:,0], sample.encoded_data[:,1], s=5, alpha = 0.1, cmap= 'turbo')\n",
    "    plt.scatter(espace_dsp_centroids[:,0], espace_dsp_centroids[:,1], marker='x')\n",
    "    \n",
    "    return espace_dsp_centroids, centroid_search_region\n",
    "\n",
    "def from_centroids_refine_clusters_and_centroids(centroids, R, sample):\n",
    "    R_closest_c = find_R_closest_centroid(R, centroids)\n",
    "    new_hull_map, probs = get_map_from_R_boundaries(R_closest_c, sample, centroids)\n",
    "    hull_r_centroids = get_centroids(sample, new_hull_map)\n",
    "    plt.figure()\n",
    "    plt.scatter(sample.encoded_data[:,0], sample.encoded_data[:,1], c=flatten_nav(new_hull_map), cmap = 'turbo')\n",
    "    plt.scatter(centroids[:,0], centroids[:,1], marker='x', c = 'grey')\n",
    "    plt.scatter(hull_r_centroids[:,0], hull_r_centroids[:,1], marker='x', c = 'black')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(new_hull_map, cmap= 'turbo')\n",
    "    return hull_r_centroids, new_hull_map, probs, R_closest_c\n",
    "\n",
    "def point_in_hull(point, hull, tolerance=1e-12):\n",
    "    return all(\n",
    "        (np.dot(eq[:-1], point) + eq[-1] <= tolerance)\n",
    "        for eq in hull.equations)\n",
    "\n",
    "def get_centroids(sample, map1):\n",
    "    centroids = [np.mean(sample.encoded_data[np.where(flatten_nav(map1) == uind)], axis = 0) for uind in np.unique(map1)]\n",
    "    return np.asarray(centroids)\n",
    "\n",
    "def get_new_centroids(centroids, map1, ssi_thresh = 1):\n",
    "    centroid_patts = get_terr_patts(centroids)\n",
    "\n",
    "    centroid_cm = np.zeros((centroids.shape[0], centroids.shape[0]))\n",
    "\n",
    "    for i in range((centroid_patts.shape[0])):\n",
    "        for j in range((centroid_patts.shape[0])):\n",
    "            if i == j:\n",
    "                centroid_cm[i][j] = 100\n",
    "            else:\n",
    "                ssi = (1- SSI(centroid_patts[i], centroid_patts[j]))*100\n",
    "                centroid_cm[i][j] = ssi\n",
    "\n",
    "\n",
    "    edges = np.asarray(np.where(centroid_cm<ssi_thresh)).T\n",
    "    nodes = np.unique(edges)\n",
    "\n",
    "    g = nx.Graph()\n",
    "\n",
    "    g.add_nodes_from(nodes)\n",
    "    g.add_edges_from(edges)\n",
    "\n",
    "    plt.figure()\n",
    "    nx.draw(g, with_labels= True)\n",
    "\n",
    "    con_comp = [x for x in nx.connected_components(g)]\n",
    "    \n",
    "\n",
    "    all_con_comps = []\n",
    "    _ = [[all_con_comps.append(i) for i in e] for e in con_comp]\n",
    "\n",
    "    uncon_comps = list(range(centroids.shape[0]))\n",
    "    _ = [uncon_comps.pop(uncon_comps.index(i)) for i in all_con_comps]\n",
    "    \n",
    "    print(all_con_comps, uncon_comps)\n",
    "\n",
    "    comb_map1 = np.zeros_like(map1)\n",
    "\n",
    "    for i,uc  in enumerate(uncon_comps):\n",
    "        comb_map1[np.where(map1 == uc)] = i\n",
    "    for j, cc in enumerate(con_comp):\n",
    "        for jc in cc:\n",
    "            comb_map1[np.where(map1 == jc)] = i+j+1\n",
    "            \n",
    "\n",
    "    new_centroids = get_centroids(sample, comb_map1)\n",
    "    plt.figure()\n",
    "    plt.scatter(sample.encoded_data[:,0], sample.encoded_data[:,1], s=5, alpha = 0.1, c = flatten_nav(comb_map1), cmap= 'turbo')\n",
    "    plt.scatter(new_centroids[:,0], new_centroids[:,1], marker='x', c='red', s = 25)\n",
    "\n",
    "    return new_centroids\n",
    "\n",
    "def find_R_closest_centroid(R, new_centroids):\n",
    "    R_patts = get_terr_patts(R)\n",
    "\n",
    "    ncp = get_terr_patts(new_centroids)\n",
    "\n",
    "    R_ssi = []\n",
    "    for R_p in R_patts:\n",
    "        R_ssi.append(np.argsort([SSI(R_p, incp) for incp in ncp]))\n",
    "\n",
    "    R_ssi = np.asarray(R_ssi)\n",
    "\n",
    "    best_centroid = R_ssi[:,-1]\n",
    "\n",
    "    centroid_Rs = []\n",
    "    for uind in np.unique(best_centroid):\n",
    "        centroid_Rs.append(R[np.where(best_centroid==uind)])\n",
    "    return np.asarray(centroid_Rs)\n",
    "\n",
    "def get_map_from_R_boundaries(centroid_Rs, sample, new_centroids):\n",
    "    hull_labels = []\n",
    "    for cind in range(len(centroid_Rs)):\n",
    "        try:\n",
    "            hull = ConvexHull(centroid_Rs[cind])\n",
    "            hull_labels.append(np.where([point_in_hull(p,hull) for p in sample.encoded_data], 1, 0))\n",
    "        except:\n",
    "            hull_labels.append(np.zeros(sample.encoded_data.shape[0]))\n",
    "    hull_labels = np.asarray(hull_labels)\n",
    "\n",
    "    hull_labels = np.asarray(hull_labels)\n",
    "\n",
    "    conflict_points = np.where(hull_labels.sum(axis = 0) > 1)[0]\n",
    "\n",
    "    non_conflict = list(range(hull_labels.shape[1]))\n",
    "    _ = [non_conflict.pop(non_conflict.index(i)) for i in conflict_points]\n",
    "\n",
    "    conf = np.ones(hull_labels.shape[0])\n",
    "            \n",
    "    ncp = get_terr_patts(new_centroids)\n",
    "    conflict_patts = get_terr_patts(sample.encoded_data[conflict_points])\n",
    "\n",
    "    conflicting_point_probs = []\n",
    "\n",
    "    for cpi, cp in enumerate(conflict_points):\n",
    "        conflicting_cents = np.where(hull_labels[:,cp] ==1)[0]\n",
    "        cp_patt = conflict_patts[cpi]\n",
    "        conflicting_ssi = []\n",
    "        for ccent in conflicting_cents:\n",
    "            conflicting_ssi.append(SSI(ncp[ccent], cp_patt))\n",
    "        conflicting_ssi = np.asarray(conflicting_ssi)\n",
    "        probabilities = conflicting_ssi/np.sum(conflicting_ssi)\n",
    "        prob_dict = {\"{}\"}\n",
    "        for i, ccent in enumerate(conflicting_cents):\n",
    "            prob_dict[ccent] = probabilities[i]\n",
    "        conflicting_point_probs.append(prob_dict)\n",
    "\n",
    "    conflicting_point_probs\n",
    "\n",
    "    hull_prediction_labels = np.zeros(sample.encoded_data.shape[0])\n",
    "\n",
    "    hull_labels.shape\n",
    "\n",
    "    np.where(hull_labels[:,0]==1)[0][0]\n",
    "\n",
    "    for nci in non_conflict:\n",
    "        clust = np.where(hull_labels[:,nci]==1)[0]\n",
    "        if len(clust) == 0:\n",
    "            best_clust = -1\n",
    "        else:\n",
    "            best_clust = clust[0]\n",
    "        hull_prediction_labels[nci] = best_clust\n",
    "\n",
    "    for i, cpi in enumerate(conflict_points):\n",
    "        probs = conflicting_point_probs[i]\n",
    "        best_fit = list(probs.keys())[np.asarray(list(probs.values())).argmax()]\n",
    "        hull_prediction_labels[cpi] = best_fit\n",
    "    \n",
    "\n",
    "    hull_labels.shape\n",
    "\n",
    "    outliers = np.where(hull_prediction_labels ==-1)[0]\n",
    "\n",
    "    outlier_patts = get_terr_patts(sample.encoded_data[outliers])\n",
    "\n",
    "    for i, p in enumerate(outlier_patts):\n",
    "        hull_prediction_labels[outliers[i]] = np.argsort([SSI(p, incp) for incp in ncp])[-1]\n",
    "\n",
    "    hull_prediction_relabelled = np.zeros_like(hull_prediction_labels)\n",
    "    for i, ind in enumerate(np.unique(hull_prediction_labels)):\n",
    "        hull_prediction_relabelled[np.where(hull_prediction_labels==ind)]=i \n",
    "        \n",
    "    nav_shape = sample.raw_data.data.shape[:2]\n",
    "\n",
    "    hull_pred_map = hull_prediction_relabelled.reshape(nav_shape)\n",
    "    return hull_pred_map, conflicting_point_probs\n",
    "\n",
    "def merge_centroids(centroids, ssi_thresh = 1):\n",
    "    centroid_patts = get_terr_patts(centroids)\n",
    "\n",
    "    centroid_cm = np.zeros((centroids.shape[0], centroids.shape[0]))\n",
    "\n",
    "    for i in range((centroid_patts.shape[0])):\n",
    "        for j in range((centroid_patts.shape[0])):\n",
    "            if i == j:\n",
    "                centroid_cm[i][j] = 100\n",
    "            else:\n",
    "                ssi = (1- SSI(centroid_patts[i], centroid_patts[j]))*100\n",
    "                centroid_cm[i][j] = ssi\n",
    "\n",
    "\n",
    "    edges = np.asarray(np.where(centroid_cm<ssi_thresh)).T\n",
    "    nodes = np.unique(edges)\n",
    "\n",
    "    g = nx.Graph()\n",
    "\n",
    "    g.add_nodes_from(nodes)\n",
    "    g.add_edges_from(edges)\n",
    "\n",
    "    plt.figure()\n",
    "    nx.draw(g, with_labels= True)\n",
    "\n",
    "    con_comp = [x for x in nx.connected_components(g)]\n",
    "    \n",
    "\n",
    "    all_con_comps = []\n",
    "    _ = [[all_con_comps.append(i) for i in e] for e in con_comp]\n",
    "\n",
    "    uncon_comps = list(range(centroids.shape[0]))\n",
    "    _ = [uncon_comps.pop(uncon_comps.index(i)) for i in all_con_comps]\n",
    "\n",
    "\n",
    "    new_centroids = []\n",
    "\n",
    "    for uc  in uncon_comps:\n",
    "        new_centroids.append(centroids[uc])\n",
    "    for j, cc in enumerate(con_comp):\n",
    "        new_centroids.append(np.mean(np.asarray([centroids[jc] for jc in cc]), axis = 0))\n",
    "            \n",
    "\n",
    "    new_centroids = np.asarray(new_centroids)\n",
    "    plt.figure()\n",
    "    plt.scatter(sample.encoded_data[:,0], sample.encoded_data[:,1], s=5, alpha = 0.1, cmap= 'turbo')\n",
    "    plt.scatter(new_centroids[:,0], new_centroids[:,1], marker='x', c='red', s = 25)\n",
    "\n",
    "    return new_centroids\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "import networkx as nx\n",
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "\n",
    "\n",
    "fdp = Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28749-1/processing/Calibrated_MD')\n",
    "\n",
    "dps = [dp for dp in fdp.walk('.hdf5', max_depth =1) if len(dp.redirect('').walk('.hdf5', max_depth = 2)) > 1]\n",
    "\n",
    "\n",
    "\n",
    "def data_manip_lowq_resized(d, central_box = 256, bs = 256):\n",
    "    pxc, pyc = d.shape[1]//2, d.shape[2]//2 \n",
    "    pxl, pxu = pxc - central_box//2, pxc + central_box//2 \n",
    "    pyl, pyu = pyc - central_box//2, pyc + central_box//2 \n",
    "    \n",
    "    d = d[:, pxl:pxu, pyl:pyu]\n",
    "    if type(d) != np.ndarray:\n",
    "        print('dask to numpy')\n",
    "        d = d.compute()\n",
    "        print('dask to numpy done')\n",
    "    print('started data manipulations')\n",
    "    #d = resize(d,(d.shape[0],128,128))\n",
    "    print('resized')\n",
    "    d = d.astype('float32')\n",
    "    for i in range(d.shape[0]):\n",
    "        d_max = np.max(d[i])\n",
    "        d[i] = d[i]/d_max\n",
    "    d = batch_resize(d, bs)\n",
    "    scaler = np.log(1001)\n",
    "    return np.log((d*1000)+1)/scaler \n",
    "\n",
    "new_dps = []\n",
    "for dp in dps:\n",
    "    if dp.redirect('RefinedModel/',1).exists() and len(dp.redirect('RefinedModel/',1).ls()) >0:\n",
    "        new_dps.append(dp)\n",
    "\n",
    "required_dps = []\n",
    "for dp in new_dps:\n",
    "    if dp.redirect('Refined_N_components/',1).exists():\n",
    "        n_comps_done = len(dp.redirect('Refined_N_components/',1).ls())\n",
    "        if n_comps_done < 4:\n",
    "            required_dps.append(dp)\n",
    "    else:\n",
    "        required_dps.append(dp)\n",
    "\n",
    "dp = Path(\"{dp}\")\n",
    "\n",
    "### Create a ProcessedSample Object\n",
    "\n",
    "sample = ProcessedSample(dp, 'Test')\n",
    "\n",
    "sample.raw_data.plot()\n",
    "\n",
    "### Pre-process the data to speed up later functions (can avoid this if there are memory constraints)\n",
    "\n",
    "\n",
    "sample.save_ml_manipulation('full_ds', data_manip_lowq_resized)\n",
    "\n",
    "## Set the hparams, can pull these out of the info dictionary\n",
    "\n",
    "hparams= {hparams}\n",
    "\n",
    "sample.set_model_data('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28034-1/processing/Models/New','cnn',hparams['LAT'],use_generic_model = True)\n",
    "\n",
    "### Check the model has built\n",
    "\n",
    "model = create_vae_model(hparams)\n",
    "\n",
    "### Load in the trained weights\n",
    "\n",
    "best_model = Path(\"{model_path}\")\n",
    "\n",
    "model.load_weights(best_model)\n",
    "\n",
    "### You now need to set the model to the sample \n",
    "\n",
    "sample.set_model(model)\n",
    "\n",
    "### You can encode the data you pre-processed earlier (or will default to sample.raw_data)\n",
    "\n",
    "sample.encode('vae',input_data_tag='full_ds', bn= 64)\n",
    "\n",
    "### You can inspect the reconstructed image and compare it to the raw data\n",
    "\n",
    "dim0_info = (0, np.floor(sample.encoded_data[:,0].min()), np.ceil(sample.encoded_data[:,0].max()),200)\n",
    "dim1_info = (1, np.floor(sample.encoded_data[:,1].min()), np.ceil(sample.encoded_data[:,1].max()),200)\n",
    "\n",
    "sample.chart_terrain(dim0_info, dim1_info)\n",
    "\n",
    "## Sample from KDE first derivative\n",
    "\n",
    "density_approx = 2\n",
    "bw = 0.025\n",
    "n_sample_points = 4000\n",
    "n_bkg_points = 0\n",
    "n_bulk_samples = 1000\n",
    "density_cutoff = -5\n",
    "sample_grid_res = 200\n",
    "gn = 0.1\n",
    "\n",
    "D = sample.encoded_data.copy()\n",
    "\n",
    "np.random.shuffle(D)\n",
    "\n",
    "D.shape\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=bw).fit(D)\n",
    "\n",
    "xgrid, ygrid = sample.terrain_grid\n",
    "\n",
    "X,Y = np.meshgrid(xgrid, ygrid)\n",
    "xy = np.vstack((X.ravel(), Y.ravel())).T\n",
    "\n",
    "Z = kde.score_samples(xy).reshape(X.shape)\n",
    "\n",
    "dY, dX = np.gradient(Z)\n",
    "\n",
    "R = get_density_gradient_net(sample.encoded_data.copy(), n_sample_points, density_cutoff, n_bkg_points,n_bulk_samples, density_approx, sample_grid_res,bw, gn)\n",
    "\n",
    "\n",
    "\n",
    "density_cutoff = -6\n",
    "\n",
    "allowed_centroid_mask = np.where(Z < density_cutoff, 0, 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(allowed_centroid_mask)\n",
    "\n",
    "bounded_locs = (np.where(allowed_centroid_mask==1)[1], np.where(allowed_centroid_mask==1)[0])\n",
    "\n",
    "bounded_regions = sample.terrain_signal.data[bounded_locs]\n",
    "\n",
    "norm_bounded_regions = bounded_regions/np.max(bounded_regions, axis = (1,2))[:,None,None]\n",
    "\n",
    "cent = pxm.signals.ElectronDiffraction2D(bounded_regions)\n",
    "\n",
    "for ncomponents in (8,16, 24,32):\n",
    "\n",
    "    cent.decomposition(True, algorithm='NMF', output_dimension=ncomponents)\n",
    "\n",
    "    decomp_facts = cent.get_decomposition_factors().data\n",
    "\n",
    "    norm_dcf = decomp_facts/ np.max(decomp_facts, axis = (1,2))[:,None,None]\n",
    "\n",
    "    br_locs = np.asarray(np.where(allowed_centroid_mask==1)).T\n",
    "\n",
    "    best_matches = [np.argmax([SSI(br, df) for br in norm_bounded_regions]) for df in norm_dcf]\n",
    "\n",
    "    pca_centroid_locs = br_locs[np.asarray(best_matches)]\n",
    "\n",
    "    pca_centroids = np.concatenate((xgrid[pca_centroid_locs[:,1]][:,None], ygrid[pca_centroid_locs[:,0]][:,None]), axis = 1)\n",
    "\n",
    "\n",
    "    t1 = time.time()\n",
    "    hull_r_centroids, new_hull_map, probs, clustered_Rs = from_centroids_refine_clusters_and_centroids(pca_centroids, R, sample)\n",
    "    print(time.time()-t1)\n",
    "\n",
    "    segfig = plt.figure(figsize = (10,10))\n",
    "    plt.imshow(new_hull_map, cmap='turbo',interpolation = 'nearest')\n",
    "\n",
    "    latfig = plt.figure(figsize = (10,10))\n",
    "    plt.scatter(sample.encoded_data[:,0], sample.encoded_data[:,1], c=flatten_nav(new_hull_map), cmap = 'turbo')\n",
    "    plt.scatter(pca_centroids[:,0], pca_centroids[:,1], marker='o', facecolors='none', edgecolors='black', lw = 2)\n",
    "    plt.scatter(hull_r_centroids[:,0], hull_r_centroids[:,1], marker='x', c = 'black', lw= 2)\n",
    "\n",
    "    compdir = dp.redirect('{folder_name}')\n",
    "    if not compdir.exists():\n",
    "        compdir.mkdir()\n",
    "    ncompdir = compdir.redirect(f'{\"{ncomponents}\"}',0)\n",
    "    if not ncompdir.exists():\n",
    "        ncompdir.mkdir()\n",
    "    ncompreg = ncompdir.redirect('Regions',0)\n",
    "    if not ncompreg.exists():\n",
    "        ncompreg.mkdir()\n",
    "\n",
    "    np.save(ncompdir.redirect(f'mapdata.npy', 0), new_hull_map)\n",
    "\n",
    "    segfig.savefig(ncompdir.redirect(f'map.jpg', 0))\n",
    "\n",
    "    latfig.savefig(ncompdir.redirect(f'latmap.jpg', 0))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(new_hull_map, interpolation = 'nearest', cmap = 'turbo')\n",
    "\n",
    "    sample.all_maps['refine1'] = new_hull_map.astype('int')\n",
    "\n",
    "    sample.get_map_patterns('refine1', method = 'mean', recompute=True)\n",
    "\n",
    "    ### View a signal boosted representation of the sample\n",
    "    print('start sbs')\n",
    "\n",
    "    sbs = signal_boosted_scan(sample, 'refine1')\n",
    "    print('done sbs')\n",
    "\n",
    "    unique_regions = [np.asarray(np.where(sample.all_maps['refine1'] == x))[:,0] for x in np.unique(sample.all_maps['refine1'])]\n",
    "    print('do regions')\n",
    "    for i, ur in enumerate(unique_regions):\n",
    "        f = inv_sbs(sbs,'refine1', ur, return_fig=True, vmax = 0.2, interactive=False)\n",
    "        f.savefig(ncompreg.redirect(f'{\"{i}\"}-vmax0.2.jpg',0), dpi = 200)\n",
    "        del f\n",
    "    print('done regions')\n",
    "\n",
    "    del decomp_facts, norm_dcf, br_locs, best_matches, pca_centroid_locs, pca_centroids, hull_r_centroids, new_hull_map, probs, clustered_Rs, segfig, latfig, sbs, unique_regions\n",
    "'''\n",
    "\n",
    "def job_submission_script(fpath, lpath):\n",
    "    return f'''\n",
    "#!/bin/bash\n",
    "#$ -l h_rt=119:00:00\n",
    "#$ -cwd\n",
    "#$ -P e02\n",
    "#$ -q all.q\n",
    "#$ -l m_mem_free=128G\n",
    "#$ -l gpu=4\n",
    "#$ -o {lpath}\n",
    "#$ -e {lpath}\n",
    "\n",
    "\n",
    "module load python/epsic3.7\n",
    "module load cuda/10.1\n",
    "python {fpath}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47f5492",
   "metadata": {},
   "source": [
    "Now we need to create a list of file paths for the datasets you want to train on.\n",
    "You can do this however you like - I like to use this walk function within a list comprehension, using the \"if\" to exclude things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81ed88c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdp = Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28034-1/processing/Merlin/Calibrated/O3_pure')\n",
    "\n",
    "dps = [dp for dp in fdp.walk('.hdf5','binned', max_depth =1) if len(dp.parent.walk('RefinedModel', max_depth =3)) ==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f2f23789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28034-1/processing/Merlin/Calibrated/O3_pure/20210305 115625/20210305_115625.hdf5'),\n",
       " Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28034-1/processing/Merlin/Calibrated/O3_pure/20210305 133334/20210305_133334.hdf5'),\n",
       " Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28034-1/processing/Merlin/Calibrated/O3_pure/20210305 135737/20210305_135737.hdf5'),\n",
       " Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28034-1/processing/Merlin/Calibrated/O3_pure/20210305 140302/20210305_140302.hdf5'),\n",
       " Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28034-1/processing/Merlin/Calibrated/O3_pure/20210305 115108/20210305_115108.hdf5'),\n",
       " Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28034-1/processing/Merlin/Calibrated/O3_pure/20210305 124516/20210305_124516.hdf5'),\n",
       " Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28034-1/processing/Merlin/Calibrated/O3_pure/20210305 142824/20210305_142824.hdf5'),\n",
       " Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28034-1/processing/Merlin/Calibrated/O3_pure/20210305 121322/20210305_121322.hdf5'),\n",
       " Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28034-1/processing/Merlin/Calibrated/O3_pure/20210305 121028/20210305_121028.hdf5'),\n",
       " Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28034-1/processing/Merlin/Calibrated/O3_pure/20210305 115400/20210305_115400.hdf5'),\n",
       " Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28034-1/processing/Merlin/Calibrated/O3_pure/20210305 120523/20210305_120523.hdf5'),\n",
       " Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28034-1/processing/Merlin/Calibrated/O3_pure/20210305 115905/20210305_115905.hdf5'),\n",
       " Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28034-1/processing/Merlin/Calibrated/O3_pure/20210305 120115/20210305_120115.hdf5'),\n",
       " Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28034-1/processing/Merlin/Calibrated/O3_pure/20210305 120827/20210305_120827.hdf5'),\n",
       " Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28034-1/processing/Merlin/Calibrated/O3_pure/20210305 121609/20210305_121609.hdf5'),\n",
       " Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28034-1/processing/Merlin/Calibrated/O3_pure/20210305 142246/20210305_142246.hdf5'),\n",
       " Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/mg28034-1/processing/Merlin/Calibrated/O3_pure/20210305 132349/20210305_132349.hdf5')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa07c9",
   "metadata": {},
   "source": [
    "Next need to define the model parameters for the model that is being trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36588439",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {'KN1':32,'KN2':64,'KN3':128, 'KN4':128, 'KN5':256,'D1':128,'D2':512,'LAT':2,'LR':0.0001, 'B':1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301f40cd",
   "metadata": {},
   "source": [
    "Need a folder name for where to save the refinements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3cac2960",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'Refined_N_components'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e0971",
   "metadata": {},
   "source": [
    "Again, define where to save these scripts and their logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4dd482c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/ClusterJobs/LoopedWorkflowScripts/mg28034/O3_pure_refined')\n",
    "log_path = script_path.redirect('logs',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "860a305c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/dls/science/groups/imaging/ePSIC_students/Andy_Bridger/ClusterJobs/LoopedWorkflowScripts/mg28034/O3_pure_refined/logs')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b98fbf",
   "metadata": {},
   "source": [
    "Define the folder you want to look in for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "418837d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = 'RefinedModel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d5c33d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dp in dps:\n",
    "    timestamp = dp.parts[-2].replace(' ', '_')\n",
    "    \n",
    "    #Find our old model for this dataset\n",
    "    \n",
    "    model_path = best_model_from_list(dp.redirect(model_folder, 1).ls())\n",
    "        \n",
    "    #Get our script and where we are going to save it \n",
    "\n",
    "    example_script = get_script_text(dp, model_path, hparams, folder_name)\n",
    "    script_py_path = script_path.redirect(f'{timestamp}_train.py', 0)\n",
    "    \n",
    "    #Get our submission script and where we are going to save that \n",
    "    \n",
    "    script_sub_path = script_py_path.redirect(f'sub_job_{timestamp}.sh')\n",
    "    script_sub = job_submission_script(script_py_path, log_path)\n",
    "    \n",
    "    #Save these scripts \n",
    "    \n",
    "    with open(script_py_path, 'w') as f:\n",
    "        f.write(example_script)\n",
    "\n",
    "    with open(script_sub_path, 'w') as f:\n",
    "        f.write(script_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f815de",
   "metadata": {},
   "source": [
    "Now submit the jobs to hamilton by running the following commands:\n",
    "> module load hamilton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb1706f",
   "metadata": {},
   "source": [
    "> for f in ./*.sh; do qsub $f; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41344e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 - EPSIC [DLS Conda]",
   "language": "python",
   "name": "conda-env-DLS_Conda-epsic3.7-kernel.json"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
